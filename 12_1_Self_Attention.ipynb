{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jonny55921/CSCI-167/blob/main/12_1_Self_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Notebook 12.1: Self Attention**\n",
        "\n",
        "This notebook builds a self-attention mechanism from scratch, as discussed in section 12.2 of the book.\n",
        "\n",
        "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
        "\n",
        "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions.\n",
        "\n"
      ],
      "metadata": {
        "id": "t9vk9Elugvmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "OLComQyvCIJ7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The self-attention mechanism maps $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ and returns $N$ outputs $\\mathbf{x}'_{n}\\in \\mathbb{R}^{D}$.  \n",
        "\n"
      ],
      "metadata": {
        "id": "9OJkkoNqCVK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed so we get the same random numbers\n",
        "np.random.seed(3)\n",
        "# Number of inputs\n",
        "N = 3\n",
        "# Number of dimensions of each input\n",
        "D = 4\n",
        "# Create an empty list\n",
        "all_x = []\n",
        "# Create elements x_n and append to list\n",
        "for n in range(N):\n",
        "  all_x.append(np.random.normal(size=(D,1)))\n",
        "# Print out the list\n",
        "print(all_x)\n"
      ],
      "metadata": {
        "id": "oAygJwLiCSri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c0cdd9e-b203-4899-be80-7f6bb5142c23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[ 1.78862847],\n",
            "       [ 0.43650985],\n",
            "       [ 0.09649747],\n",
            "       [-1.8634927 ]]), array([[-0.2773882 ],\n",
            "       [-0.35475898],\n",
            "       [-0.08274148],\n",
            "       [-0.62700068]]), array([[-0.04381817],\n",
            "       [-0.47721803],\n",
            "       [-1.31386475],\n",
            "       [ 0.88462238]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also need the weights and biases for the keys, queries, and values (equations 12.2 and 12.4)"
      ],
      "metadata": {
        "id": "W2iHFbtKMaDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed so we get the same random numbers\n",
        "np.random.seed(0)\n",
        "\n",
        "# Choose random values for the parameters\n",
        "omega_q = np.random.normal(size=(D,D))\n",
        "omega_k = np.random.normal(size=(D,D))\n",
        "omega_v = np.random.normal(size=(D,D))\n",
        "beta_q = np.random.normal(size=(D,1))\n",
        "beta_k = np.random.normal(size=(D,1))\n",
        "beta_v = np.random.normal(size=(D,1))"
      ],
      "metadata": {
        "id": "79TSK7oLMobe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's compute the queries, keys, and values for each input"
      ],
      "metadata": {
        "id": "VxaKQtP3Ng6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make three lists to store queries, keys, and values\n",
        "all_queries = []\n",
        "all_keys = []\n",
        "all_values = []\n",
        "# For every input\n",
        "for x in all_x:\n",
        "  # TODO -- compute the keys, queries and values.\n",
        "  # Replace these three lines\n",
        "  query = np.dot(omega_q, x) + beta_q\n",
        "  key = np.dot(omega_k, x) + beta_k\n",
        "  value = np.dot(omega_v, x) + beta_v\n",
        "\n",
        "  all_queries.append(query)\n",
        "  all_keys.append(key)\n",
        "  all_values.append(value)"
      ],
      "metadata": {
        "id": "TwDK2tfdNmw9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need a softmax function (equation 12.5) -- here, it will take a list of arbitrary numbers and return a list where the elements are non-negative and sum to one\n"
      ],
      "metadata": {
        "id": "Se7DK6PGPSUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(items_in):\n",
        "\n",
        "  # TODO Compute the elements of items_out\n",
        "  # Replace this line\n",
        "  exp_values = np.exp(items_in)\n",
        "  # Sum over the values\n",
        "  denom = np.sum(exp_values)\n",
        "  # Compute softmax\n",
        "  items_out = exp_values / denom\n",
        "\n",
        "  return items_out ;"
      ],
      "metadata": {
        "id": "u93LIcE5PoiM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now compute the self attention values:"
      ],
      "metadata": {
        "id": "8aJVhbKDW7lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create emptymlist for output\n",
        "all_x_prime = []\n",
        "\n",
        "# For each output\n",
        "for n in range(N):\n",
        "  # Create list for dot products of query N with all keys\n",
        "  all_km_qn = []\n",
        "  # Compute the dot products\n",
        "  for key in all_keys:\n",
        "    # Compute the appropriate dot product\n",
        "    dot_product = np.dot(all_queries[n].T, key)\n",
        "\n",
        "    # Store dot product\n",
        "    all_km_qn.append(dot_product[0,0])  # Extract scalar value\n",
        "\n",
        "  # Compute dot product\n",
        "  attention = softmax(all_km_qn)\n",
        "  # Print result (should be positive sum to one)\n",
        "  print(\"Attentions for output \", n)\n",
        "  print(attention)\n",
        "\n",
        "  # TODO: Compute a weighted sum of all of the values according to the attention\n",
        "  # (equation 12.3)\n",
        "  # Replace this line\n",
        "  x_prime = np.zeros_like(all_values[0])  # Initialize x_prime\n",
        "\n",
        "  for i in range(len(all_values)):\n",
        "    x_prime += attention[i] * all_values[i]\n",
        "\n",
        "  all_x_prime.append(x_prime)\n",
        "\n",
        "\n",
        "# Print out true values to check you have it correct\n",
        "print(\"x_prime_0_calculated:\", all_x_prime[0].transpose())\n",
        "print(\"x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\")\n",
        "print(\"x_prime_1_calculated:\", all_x_prime[1].transpose())\n",
        "print(\"x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\")\n",
        "print(\"x_prime_2_calculated:\", all_x_prime[2].transpose())\n",
        "print(\"x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\")\n"
      ],
      "metadata": {
        "id": "yimz-5nCW6vQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdf2be9e-a8e5-4c55-f26d-818ca860452f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attentions for output  0\n",
            "[1.24326146e-13 9.98281489e-01 1.71851130e-03]\n",
            "Attentions for output  1\n",
            "[2.79525306e-12 5.85506360e-03 9.94144936e-01]\n",
            "Attentions for output  2\n",
            "[0.00505708 0.00654776 0.98839516]\n",
            "x_prime_0_calculated: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\n",
            "x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\n",
            "x_prime_1_calculated: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\n",
            "x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\n",
            "x_prime_2_calculated: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\n",
            "x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's compute the same thing, but using matrix calculations.  We'll store the $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ in the columns of a $D\\times N$ matrix, using equations 12.6 and 12.7/8.\n",
        "\n",
        "Note:  The book uses column vectors (for compatibility with the rest of the text), but in the wider literature it is more normal to store the inputs in the rows of a matrix;  in this case, the computation is the same, but all the matrices are transposed and the operations proceed in the reverse order."
      ],
      "metadata": {
        "id": "PJ2vCQ_7C38K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define softmax operation that works independently on each column\n",
        "def softmax_cols(data_in):\n",
        "    # Get the max value per column for numerical stability\n",
        "    max_values = np.max(data_in, axis=0)\n",
        "    shifted_data = data_in - max_values  # Broadcasting subtraction\n",
        "\n",
        "    # Exponentiate the shifted data\n",
        "    exp_values = np.exp(shifted_data)\n",
        "\n",
        "    # Sum over columns (axis 0)\n",
        "    denom = np.sum(exp_values, axis=0)\n",
        "\n",
        "    # Normalize by the sum (broadcasted division)\n",
        "    softmax = exp_values / denom\n",
        "\n",
        "    return softmax"
      ],
      "metadata": {
        "id": "obaQBdUAMXXv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Now let's compute self attention in matrix form\n",
        "def self_attention(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
        "    # Compute queries, keys, and values\n",
        "    Queries = np.dot(omega_q, X) + beta_q\n",
        "    Keys = np.dot(omega_k, X) + beta_k\n",
        "    Values = np.dot(omega_v, X) + beta_v\n",
        "\n",
        "    # Compute dot products (unnormalized attentions)\n",
        "    DotProducts = np.dot(Queries.T, Keys)\n",
        "\n",
        "    # Scale the dot product (for stability in softmax)\n",
        "    D_k = Keys.shape[0]  # Dimensionality of keys\n",
        "    ScaledDotProduct = DotProducts / np.sqrt(D_k)\n",
        "\n",
        "    # Apply softmax to calculate attentions\n",
        "    Attentions = softmax_cols(ScaledDotProduct)\n",
        "\n",
        "    # Weight values by attentions (Corrected)\n",
        "    X_prime = np.dot(Values, Attentions)\n",
        "\n",
        "    return X_prime"
      ],
      "metadata": {
        "id": "gb2WvQ3SiH8r"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy data into matrix\n",
        "X = np.zeros((D, N))\n",
        "X[:,0] = np.squeeze(all_x[0])\n",
        "X[:,1] = np.squeeze(all_x[1])\n",
        "X[:,2] = np.squeeze(all_x[2])\n",
        "\n",
        "# Run the self attention mechanism\n",
        "X_prime = self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
        "\n",
        "# Print out the results\n",
        "print(X_prime)"
      ],
      "metadata": {
        "id": "MUOJbgJskUpl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e557b50-5d96-4db0-ec8f-e6e94b077097"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.64608331  0.66149223  1.21854105]\n",
            " [-0.08376846  0.36015254 -0.17896151]\n",
            " [ 4.05660686 -0.05852563  1.02632615]\n",
            " [ 2.20233961  0.09885586  0.58832268]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you did this correctly, the values should be the same as above.\n",
        "\n",
        "TODO:  \n",
        "\n",
        "Print out the attention matrix\n",
        "You will see that the values are quite extreme (one is very close to one and the others are very close to zero.  Now we'll fix this problem by using scaled dot-product attention."
      ],
      "metadata": {
        "id": "as_lRKQFpvz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's compute self attention in matrix form\n",
        "def scaled_dot_product_self_attention(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
        "    # 1. Compute queries, keys, and values\n",
        "    Queries = np.dot(omega_q, X) + beta_q  # Shape: (N, D_q)\n",
        "    Keys = np.dot(omega_k, X) + beta_k    # Shape: (N, D_k)\n",
        "    Values = np.dot(omega_v, X) + beta_v  # Shape: (N, D_v)\n",
        "\n",
        "    # 2. Compute dot products (unnormalized attention scores)\n",
        "    DotProducts = np.dot(Queries.T, Keys)  # Shape: (D_q, N) . (N, D_k) => (D_q, D_k)\n",
        "\n",
        "    # 3. Scale the dot products (as in equation 12.9)\n",
        "    D_k = omega_k.shape[0]  # Dimensionality of the key vectors\n",
        "    ScaledDotProducts = DotProducts / np.sqrt(D_k)  # Scaling factor\n",
        "\n",
        "    # 4. Apply softmax to calculate attentions (normalize by columns)\n",
        "    Attentions = softmax_cols(ScaledDotProducts)\n",
        "\n",
        "    # 5. Weight values by attentions (final weighted sum)\n",
        "    X_prime = np.dot(Values, Attentions)  # Shape: (D_v, N) . (N, N) => (D_v, N)\n",
        "\n",
        "    return X_prime"
      ],
      "metadata": {
        "id": "kLU7PUnnqvIh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the self attention mechanism\n",
        "X_prime = scaled_dot_product_self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
        "\n",
        "# Print out the results\n",
        "print(X_prime)"
      ],
      "metadata": {
        "id": "n18e3XNzmVgL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b85fe60-2e16-40c0-c7d8-9f0938bd75e6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.64608331  0.66149223  1.21854105]\n",
            " [-0.08376846  0.36015254 -0.17896151]\n",
            " [ 4.05660686 -0.05852563  1.02632615]\n",
            " [ 2.20233961  0.09885586  0.58832268]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO -- Investigate whether the self-attention mechanism is covariant with respect to permutation.\n",
        "If it is, when we permute the columns of the input matrix $\\mathbf{X}$, the columns of the output matrix $\\mathbf{X}'$ will also be permuted.\n",
        "\n",
        "In the code below, we reuse our softmax and attention functions as well as a sample array to check for covariance with permutations. As you can see below after permutating the array, covariance changes. The values between the two final arrays are quite different as we can see.  \n"
      ],
      "metadata": {
        "id": "QDEkIrcgrql-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a simple permutation matrix for testing (example: swap columns 1 and 3)\n",
        "P = np.array([[0, 0, 1],\n",
        "              [1, 0, 0],\n",
        "              [0, 1, 0]])\n",
        "\n",
        "# Define a random input matrix X (e.g., 3 examples, 3 features)\n",
        "X = np.random.randn(3, 3)\n",
        "\n",
        "# Define omega matrices and beta vectors (e.g., randomly)\n",
        "omega_v = np.random.randn(3, 3)\n",
        "omega_q = np.random.randn(3, 3)\n",
        "omega_k = np.random.randn(3, 3)\n",
        "beta_v = np.random.randn(3)\n",
        "beta_q = np.random.randn(3)\n",
        "beta_k = np.random.randn(3)\n",
        "\n",
        "# Define the self-attention function (scaled dot product as defined earlier)\n",
        "def softmax_cols(data_in):\n",
        "    max_values = np.max(data_in, axis=0)  # Get max for numerical stability\n",
        "    shifted_data = data_in - max_values  # Shift data\n",
        "    exp_values = np.exp(shifted_data)  # Exponentiate the shifted data\n",
        "    denom = np.sum(exp_values, axis=0)  # Sum over columns\n",
        "    softmax = exp_values / denom  # Normalize by the sum\n",
        "    return softmax\n",
        "\n",
        "def scaled_dot_product_self_attention(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
        "    Queries = np.dot(omega_q, X) + beta_q  # Shape: (N, D_q)\n",
        "    Keys = np.dot(omega_k, X) + beta_k    # Shape: (N, D_k)\n",
        "    Values = np.dot(omega_v, X) + beta_v  # Shape: (N, D_v)\n",
        "    DotProducts = np.dot(Queries.T, Keys)  # Shape: (D_q, N) . (N, D_k) => (D_q, D_k)\n",
        "    D_k = omega_k.shape[0]  # Dimensionality of the key vectors\n",
        "    ScaledDotProducts = DotProducts / np.sqrt(D_k)  # Scaling factor\n",
        "    Attentions = softmax_cols(ScaledDotProducts)\n",
        "    X_prime = np.dot(Values, Attentions)  # Shape: (D_v, N) . (N, N) => (D_v, N)\n",
        "    return X_prime\n",
        "\n",
        "# Apply attention to original X\n",
        "X_prime = scaled_dot_product_self_attention(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
        "\n",
        "# Permute the columns of X using matrix P\n",
        "X_permuted = np.dot(P, X)\n",
        "\n",
        "# Apply attention to permuted X\n",
        "X_prime_permuted = scaled_dot_product_self_attention(X_permuted, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
        "\n",
        "print(\"Original Input Matrix X:\")\n",
        "print(X)\n",
        "print(\"\\nPermuted Input Matrix X_P (using permutation matrix P):\")\n",
        "print(X_permuted)\n",
        "\n",
        "# Print the outputs from self-attention\n",
        "print(\"\\nOutput of self-attention on original X (X'):\")\n",
        "print(X_prime)\n",
        "\n",
        "print(\"\\nOutput of self-attention on permuted X (X_P'):\")\n",
        "print(X_prime_permuted)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVu9PDuzWTO2",
        "outputId": "5908758a-ecc5-46ee-d0a7-589bf9c87885"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Input Matrix X:\n",
            "[[-0.17154633  0.77179055  0.82350415]\n",
            " [ 2.16323595  1.33652795 -0.36918184]\n",
            " [-0.23937918  1.0996596   0.65526373]]\n",
            "\n",
            "Permuted Input Matrix X_P (using permutation matrix P):\n",
            "[[-0.23937918  1.0996596   0.65526373]\n",
            " [-0.17154633  0.77179055  0.82350415]\n",
            " [ 2.16323595  1.33652795 -0.36918184]]\n",
            "\n",
            "Output of self-attention on original X (X'):\n",
            "[[-4.09453714 -3.55665784 -2.77267499]\n",
            " [ 0.26199143 -0.05176547 -0.41585387]\n",
            " [-0.15097688  0.30390862  0.86032896]]\n",
            "\n",
            "Output of self-attention on permuted X (X_P'):\n",
            "[[-0.42299352 -0.48805159 -0.71898878]\n",
            " [-0.57817855 -0.64222301 -0.44361882]\n",
            " [ 0.93680422  1.01310935  1.26948369]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "whUzlqtZWYy_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}